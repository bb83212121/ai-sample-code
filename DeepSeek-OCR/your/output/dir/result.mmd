
Inference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8: 


torch==2.6.0 


transformers==4.46.3 


tokenizers==0.20.3 


einops 


addict 


easydict 


pip install flash-attn==2.7.3 --no-build-isolation